{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d34908a-540c-4431-91bc-f34e2554ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from util import load_results, load_specific_results, store_jsonl\n",
    "from const import model_name_dict, model_name_to_path, dataset_model_best_lr, LETTERS, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4787ea9-29a9-4673-8ca7-91ef931f33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a39232-c8bd-45f9-9b68-45d4063b3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group results by instance IDs\n",
    "def group_individual_results(some_results):\n",
    "    grouped_results = {}\n",
    "\n",
    "    for instance in some_results:\n",
    "        q = instance['question']\n",
    "        if q not in grouped_results:\n",
    "            grouped_results[q] = []\n",
    "        grouped_results[q].append(instance)\n",
    "    \n",
    "    return grouped_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abdb61dd-4754-416f-b6dc-5db7fadbf5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    }
   ],
   "source": [
    "# Instances where the NoCoT & CoT predictions agree beforehand\n",
    "def filter_for_agreement(results):\n",
    "    return {\n",
    "        k:r for k, r in results.items() if r[0]['prediction'] == r[0]['cot_prediction']\n",
    "    }\n",
    "\n",
    "filtered_results = filter_for_agreement(grouped_results)\n",
    "print(len(filtered_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c907a7c-81b3-4b94-8a84-12f965fac88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changed_prediction(step_results):\n",
    "    per_step_changed = []\n",
    "    for unlearned_step in step_results:\n",
    "        ch, _ = step_changed_prediction(unlearned_step)\n",
    "        per_step_changed.append(ch) \n",
    "    return per_step_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f36f6a5b-33e4-4049-a83e-786a2f86cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_changed_prediction(a_result):\n",
    "    unlearning_results = a_result['unlearning_results']\n",
    "    preds = [np.argmax(r['probs']) for _, r in unlearning_results.items()]\n",
    "    step_changes = [p != preds[0] for p in preds[1:]]\n",
    "    \n",
    "    per_step_changed = any(step_changes)\n",
    "    return per_step_changed, step_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12199dfa-1ead-4fb1-af9a-e2882ab7f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_flips(results):\n",
    "    return {\n",
    "        k:r for k,r in results.items() if not any(changed_prediction(r))\n",
    "    }\n",
    "    \n",
    "def has_flips(results):\n",
    "    return {\n",
    "        k:r for k,r in results.items() if any(changed_prediction(r))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e56c58c3-636f-4ad8-9ed6-c757fd469105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agreement_after(a_result):\n",
    "    stepwise_results = a_result['unlearning_results']\n",
    "    iterwise_agreement = [\n",
    "        np.argmax(rr['probs']) == np.argmax(rr['new_cot_probs']) for _, rr in stepwise_results.items()\n",
    "    ]\n",
    "    iterwise_preds = [(LETTERS[np.argmax(rr['probs'])], LETTERS[np.argmax(rr['new_cot_probs'])]) for _, rr in stepwise_results.items()]\n",
    "    return iterwise_agreement, iterwise_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d95d4182-4564-4ebf-93bc-d6f0d01c0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_agreement_after(results):\n",
    "    samples = {}\n",
    "    removed_steps = 0\n",
    "    total_steps = 0\n",
    "    for k, inst in results.items():\n",
    "        sample_results = []\n",
    "        for step, step_results in enumerate(inst):\n",
    "            step_changed, step_changes = step_changed_prediction(step_results)\n",
    "            if not step_changed: continue\n",
    "            \n",
    "            step_agreement, _ = agreement_after(step_results)\n",
    "            n_post_agreement = sum(step_agreement)\n",
    "\n",
    "            if n_post_agreement >= 2 and sum(step_changes) >= 2 and step_agreement[-1] and step_changes[-1]:\n",
    "                sample_results.append(step_results)\n",
    "            else:\n",
    "                removed_steps += 1\n",
    "            total_steps += 1\n",
    "\n",
    "        samples[k] = sample_results\n",
    "    print(f\"Removed {removed_steps} steps out of {total_steps}\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b64af17b-7be5-4d80-9fcb-1b10be00c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "PROMPT_PREFIX = \"\"\"You are given a question, the answer options, and two reasoning chains.\n",
    "Your task is to assess whether the reasoning chains argue for the same answer option or not.\n",
    "In case they argue for the same option, output only \"Yes\", in case they support different options, answer \"No\", while if the answer is unclear output \"Unclear\".\n",
    "In the next line, output a short description (one sentence) explaining why you gave that answer. \n",
    "\n",
    "Question: {q}\n",
    "Answer options:\n",
    "{o}\n",
    "\n",
    "Reasoning chain 1:\n",
    "{cot_1}\n",
    "\n",
    "Reasoning chain 2:\n",
    "{cot_2}\n",
    "\n",
    "Do the reasoning chains argue for the same answer option?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def format_prompt(q, o, step_before, step_after):\n",
    "    coinflip = random.randint(0,1)\n",
    "    cot_1 = step_before if coinflip else step_after\n",
    "    cot_2 = step_after if coinflip else step_before\n",
    "    \n",
    "    return PROMPT_PREFIX.format(\n",
    "        q = q,\n",
    "        o = o,\n",
    "        cot_1 = cot_1,\n",
    "        cot_2 = cot_2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87268776-3123-4743-878d-4fbf14cce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(prompt, client, model=\"gpt-4o-mini\"):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }],\n",
    "        model=model,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dccb8d35-1433-4623-a36a-812007cd966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_judgements(the_results):\n",
    "    LM_as_judgements = {}\n",
    "    for k, inst in tqdm(changed_agree_after.items()):\n",
    "        for step_results in inst:\n",
    "            q = step_results['question']\n",
    "            options = step_results['options']\n",
    "            step_idx = step_results['step_idx']\n",
    "            target_id = f\"{q}_{step_idx}\"\n",
    "    \n",
    "            if target_id in LM_as_judgements: continue\n",
    "    \n",
    "            target_step = step_results['cot_step']\n",
    "            initial_cot = step_results['initial_cot']\n",
    "            # We know that last two unlearning steps both agree and are flipped\n",
    "    \n",
    "            final_step = step_results['unlearning_results']['5']\n",
    "            final_step_cot = final_step['new_cot']\n",
    "      \n",
    "            LM_prompt = format_prompt(q, options, initial_cot, final_step_cot)\n",
    "    \n",
    "            response = query_api(\n",
    "              LM_prompt,\n",
    "              client=client)\n",
    "            answer = response.choices[0].message.content\n",
    "            model = response.model\n",
    "            LM_as_judgements[target_id] = {\n",
    "                'prompt': LM_prompt,\n",
    "                'response': answer,\n",
    "                'model': model\n",
    "            }\n",
    "    return LM_as_judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b55e1e1-e4a4-409a-9b4b-501357618f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Store the results as json\n",
    "def dict_to_list_dict(a_dict):\n",
    "    a_list = []\n",
    "    for k, v in a_dict.items():\n",
    "        vv = deepcopy(v)        \n",
    "        vv['instance_id'] = k\n",
    "        a_list.append(vv)\n",
    "    return a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3e315ba6-af96-4c54-bfee-1934a25b86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(LM_as_judgements):\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    unk = 0\n",
    "    total = len(LM_as_judgements)\n",
    "    \n",
    "    for i, o in LM_as_judgements.items():\n",
    "        LM_answer, LM_explanation = o['response'].split(\"\\n\", 2)\n",
    "        if LM_answer.strip() == 'No': no += 1\n",
    "        elif LM_answer.strip() == 'Yes': yes += 1\n",
    "        elif LM_answer.strip() == 'Unclear': unk += 1\n",
    "        else: print(LM_answer)\n",
    "    \n",
    "    print(f\"{no}/{total}\")\n",
    "    print(f\"{yes}/{total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb829a3a-e4a0-406c-9c24-b6c2d842b2cf",
   "metadata": {},
   "source": [
    "## 3. Fetch the CoTs before and after unlearning for these instances and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b6df3-3e1a-4e18-9a3d-1b4e722c5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_name_to_path.values(): # , 'meta-llama/Meta-Llama-3-8B-Instruct', \n",
    "    model_name = model_name_dict[model.split(\"/\")[1]]\n",
    "    for dataset in datasets:\n",
    "        lr = dataset_model_best_lr[dataset][model_name]\n",
    "\n",
    "        print(f\"Running for {dataset} & {model_name}\")\n",
    "        results = load_specific_results(model_name, dataset, lr)\n",
    "        grouped_results = group_individual_results(results)\n",
    "        print(len(grouped_results))\n",
    "        changed_results = has_flips(grouped_results)\n",
    "        print(len(changed_results))\n",
    "        changed_agree_after = filter_for_agreement_after(changed_results)\n",
    "        print(len(changed_agree_after))\n",
    "\n",
    "        LM_as_judgements = generate_judgements(changed_agree_after)\n",
    "        compute_stats(LM_as_judgements)\n",
    "        results_as_list = dict_to_list_dict(LM_as_judgements)\n",
    "        print(results_as_list[0])\n",
    "        path_to_store = f\"LM_judge_cot/{model_name}_{dataset}_NPO_KL_{lr}_judgements.jsonl\"\n",
    "        print(path_to_store)\n",
    "        store_jsonl(results_as_list, path_to_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
